{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3063 Mathematics and Programming for AI\n",
    "## Resit Coursework\n",
    "\n",
    "### 1. Introduction\n",
    "This program is a simple Neural Network that evaluates the accuracy of a training set and testing set, with the implementation of three kinds of activation methods (Sigmoid, ReLu, LeakyReLu), Softmax and Inverted Dropout.\n",
    "\n",
    "This program is made using Python 3 through Jupyter Notebook IDE. \n",
    "\n",
    "### 2. Instructions\n",
    "\n",
    "Best way to run this program is to restart the whole kernel, as the cells follow a sequential routine.\n",
    "\n",
    "For re-running the program without restarting the whole kernel, after the initial run, run the cells from Loading Data and Training NN and below.\n",
    "\n",
    "Youâ€™ll require both the fashion-mnist_test.zip and fashion-mnist_train.zip file for this to run, this can be downloaded from the Google Drive linked below.\n",
    "\n",
    "#### Google Drive\n",
    "https://drive.google.com/drive/folders/1s2Ni2n0RjN2gfEagrqMHpTigm511qxLO?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import Libraries\n",
    "\n",
    "Necessary libraries that are used for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import zipfile as zp\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as plticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extracting Data and Pickling\n",
    "\n",
    "Extract compressed datasets from Zip Files and then proceed to package the data together into a pickle (.pkl) file for easier extraction of data.\n",
    "\n",
    "###### Make sure to run this at least once before running the below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('t10k-images-idx3-ubyte', 'rb') as f_in:\n",
    "#     with gzip.open('t10k-images-idx3-ubyte.gz', 'wb') as f_out:\n",
    "#         shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "num_of_labels = 10\n",
    "\n",
    "# Assign Zip Files to variable\n",
    "zf = zp.ZipFile('fashion-mnist_train.zip')\n",
    "zf1 = zp.ZipFile('fashion-mnist_test.zip')\n",
    "\n",
    "# Load CSV files from Zip Files\n",
    "train_data = np.loadtxt(zf.open('fashion-mnist_train.csv'), delimiter=',')\n",
    "test_data = np.loadtxt(zf1.open('fashion-mnist_test.csv'), delimiter=',')\n",
    "\n",
    "# Display images from 1 to 10\n",
    "# for i in range(10):\n",
    "#     img = train_imgs[i].reshape((28,28))\n",
    "#     plt.imshow(img, cmap=\"Greys\")\n",
    "#     plt.show()\n",
    "\n",
    "# Map image data values into intervals [0.01, 0.99]\n",
    "fac = 0.99 / 255\n",
    "add_fac = 0.01\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + add_fac\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) *fac + add_fac\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])\n",
    "\n",
    "lr = np.arange(num_of_labels)\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "\n",
    "# Create Pickle file from previous data\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"bw\") as fh:\n",
    "    data = (train_imgs, \n",
    "            test_imgs, \n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            train_labels_one_hot,\n",
    "            test_labels_one_hot)\n",
    "    pickle.dump(data, fh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Activation Functions, Softmax, and NN Class\n",
    "#### Task 1, 2, 3, 4\n",
    "\n",
    "We have the three activation functions: sigmoid, relu, and leakyrelu. Softmax function necessary for Task 3. Neural Network class with selectable activation functions. L2 Regularization in the form of MSE, Mean Squared Error.\n",
    "The base for this code comes from the Tutorial 5 part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x * (1 - x)\n",
    "            \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def dsoftmax(x, y):\n",
    "    si_sj = -x * x.reshape(y, 1)\n",
    "    s_der = np.diag(x) + si_sj\n",
    "    return s_der\n",
    "    \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.where(x >= 0, x, x * 0.01)\n",
    "\n",
    "def dleaky_relu(x):\n",
    "    out = np.ones_like(x)\n",
    "    out[x < 0] *= 0.01\n",
    "    return out\n",
    "\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd, \n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 activation_function,\n",
    "                 learning_rate,\n",
    "                 bias=None,\n",
    "                ):  \n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        \n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation_ih = sigmoid\n",
    "            self.activation_ho = sigmoid\n",
    "            self.dactivation_oh = dsigmoid\n",
    "            self.dactivation_hi = dsigmoid\n",
    "            \n",
    "        if activation_function == 'softmax':\n",
    "            self.activation_ih = sigmoid\n",
    "            self.activation_ho = softmax\n",
    "            self.dactivation_oh = dsoftmax\n",
    "            self.dactivation_hi = dsigmoid\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation_ih = relu\n",
    "            self.activation_ho = relu\n",
    "            self.dactivation_oh = drelu\n",
    "            self.dactivation_hi = drelu\n",
    "            \n",
    "        if activation_function == 'leakyrelu':\n",
    "            self.activation_ih = leaky_relu\n",
    "            self.activation_ho = leaky_relu\n",
    "            self.dactivation_oh = dleaky_relu\n",
    "            self.dactivation_hi = dleaky_relu\n",
    "        \n",
    "        self.use_dropout = False\n",
    "        self.active_input_percentage = 1.0\n",
    "        self.active_hidden_percentage = 1.0\n",
    "            \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes, \n",
    "                                                   self.no_of_in_nodes + bias_node))\n",
    "\n",
    "        n = (self.no_of_hidden_nodes + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes + bias_node)))\n",
    "\n",
    "    def dropout_weight_matrices(self,\n",
    "                                active_input_percentage=1.0,\n",
    "                                active_hidden_percentage=1.0):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        self.no_of_hidden_nodes_orig = self.no_of_hidden_nodes\n",
    "        self.who_orig = self.who.copy()\n",
    "        \n",
    "\n",
    "        active_input_nodes = int(self.no_of_in_nodes * active_input_percentage)\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), \n",
    "                                      active_input_nodes))\n",
    "        active_hidden_nodes = int(self.no_of_hidden_nodes * active_hidden_percentage)\n",
    "        active_hidden_indices = sorted(random.sample(range(0, self.no_of_hidden_nodes), \n",
    "                                       active_hidden_nodes))\n",
    "        \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices]       \n",
    "        self.who = self.who[:, active_hidden_indices]\n",
    "        \n",
    "        self.no_of_hidden_nodes = active_hidden_nodes\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        return active_input_indices, active_hidden_indices\n",
    "    \n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices):\n",
    "        \n",
    "        # resets weight matrices back to original\n",
    "        # self.wih and self.who variables are updated from the active nodes\n",
    " \n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "\n",
    "        self.who_orig[:, active_hidden_indices] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes = self.no_of_hidden_nodes_orig\n",
    "        \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        # Forward Propagation\n",
    "        # Input Layer to Hidden Layer\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "\n",
    "        output_vector1 = np.dot(self.wih, input_vector) # linear\n",
    "        output_vector_hidden = self.activation_ih(output_vector1) # non-linear\n",
    "        \n",
    "        # Hidden Layer to Output Layer\n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, output_vector_hidden) # linear\n",
    "        output_vector_network = self.activation_ho(output_vector2) # non-linear\n",
    "        \n",
    "        # Backward Propagation\n",
    "        # Output Layer to Hidden Layer\n",
    "        output_errors = target_vector - output_vector_network\n",
    "        \n",
    "        # if using softmax\n",
    "        if self.dactivation_oh == dsoftmax:\n",
    "            ovn = output_vector_network.reshape(output_vector_network.size,)\n",
    "            dsoftmax_output = dsoftmax(ovn, self.no_of_out_nodes)\n",
    "            tmp = np.dot(dsoftmax_output, output_errors)\n",
    "            \n",
    "        else:\n",
    "            tmp = output_errors * self.dactivation_oh(output_vector_network) # derivative\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            tmp = tmp / self.active_hidden_percentage # Inverted Dropout\n",
    "            \n",
    "        tmp = self.learning_rate  * np.dot(tmp, output_vector_hidden.T)\n",
    "        self.who += tmp # update hidden-output weights\n",
    "\n",
    "        # Hidden Layer to Input Layer\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * self.dactivation_hi(output_vector_hidden) # derivative\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            tmp = tmp / self.active_input_percentage # Inverted Dropout\n",
    "            \n",
    "        if self.bias:\n",
    "            x = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            x = np.dot(tmp, input_vector.T)\n",
    "        self.wih += self.learning_rate * x # update input-hidden weights\n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              active_input_percentage=1.0,\n",
    "              active_hidden_percentage=1.0,\n",
    "              no_of_dropout_tests = 1, \n",
    "              use_dropout = False):\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        self.active_input_percentage = active_input_percentage\n",
    "        self.active_hidden_percentage = active_hidden_percentage\n",
    "\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests)\n",
    "        \n",
    "        # if using dropout\n",
    "        if self.use_dropout:\n",
    "            for epoch in range(epochs):\n",
    "                print(\"epoch: \", epoch)\n",
    "                for start in range(0, len(data_array), partition_length):\n",
    "                    active_in_indices, active_hidden_indices = \\\n",
    "                               self.dropout_weight_matrices(active_input_percentage,\n",
    "                                                            active_hidden_percentage)\n",
    "                    for i in range(start, start + partition_length):\n",
    "                        self.train_single(data_array[i][active_in_indices], \n",
    "                                         labels_one_hot_array[i]) \n",
    "\n",
    "                    self.weight_matrices_reset(active_in_indices, active_hidden_indices)\n",
    "        else:\n",
    "            for epoch in range(epochs):\n",
    "                print(\"epoch: \", epoch)\n",
    "                for i in range(len(data_array)):\n",
    "                        self.train_single(data_array[i], labels_one_hot_array[i])\n",
    "                        \n",
    "                \n",
    "                    \n",
    "        \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = np.zeros((10, 10), int)\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            cm[res_max, int(target)] += 1\n",
    "        return cm\n",
    "    \n",
    "    def precision(self, label, confusion_matrix):\n",
    "        col = confusion_matrix[:, label]\n",
    "        return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "    def recall(self, label, confusion_matrix):\n",
    "        row = confusion_matrix[label, :]\n",
    "        return confusion_matrix[label, label] / row.sum()\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        \"\"\" input_vector can be tuple, list or ndarray \"\"\"\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        output_vector = np.dot(self.wih, \n",
    "                               input_vector)\n",
    "        output_vector = self.activation_ih(output_vector)\n",
    "        \n",
    "        output_vector = np.dot(self.who, \n",
    "                               output_vector)\n",
    "        output_vector = self.activation_ho(output_vector)\n",
    "    \n",
    "        return output_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "    \n",
    "    def mean_squared_error(self, data, labels):\n",
    "        actual = labels\n",
    "        predicted = [None] * len(actual)\n",
    "        \n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            predicted[i] = res_max\n",
    "        \n",
    "        sum_square_error = 0.0\n",
    "        for i in range(len(actual)):\n",
    "            sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "        mean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "        return mean_square_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Loading data and Training NN\n",
    "\n",
    "Load the data from the pickle file, initialise the Neural Network with the desired arguments and then train them.\n",
    "\n",
    "Notable arguments:\n",
    "\n",
    "    no_of_in_nodes - original image data size (28x28) being passed through equals input size\n",
    "    no_of_out_nodes - desired amount of output nodes for the data\n",
    "    no_of_hidden_nodes - desired number of hidden nodes to process the data\n",
    "    learning_rate - the desired learning rate\n",
    "    activation_function - the desired activation functions; \n",
    "        includes: Sigmoid 'sigmoid', ReLU 'relu', and Leaky ReLU 'leakyrelu'\n",
    "\n",
    "To use different activation functions for the Neural Network, initialise a new NeuralNetwork class and set the value of the activation_function variable as 'sigmoid' for Sigmoid, 'relu' for ReLu, and 'leakyrelu' for LeakyReLu.\n",
    "\n",
    "For Softmax pass the activation_function variable, when initialising the Neural Network, to 'softmax'.\n",
    "\n",
    "To use dropout, when running the Train set the use_dropout variable to True and change the active_input_percentage, active_hidden_percentage, and no_of_dropout_tests accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Pickle file\n",
    "with open(os.path.join(\".\",\"pkl_fashionmnist.pkl\"), \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "img_size = 28 # dimensions\n",
    "num_of_labels = 10 # 0, 1, 2, ... 9\n",
    "image_pixels = img_size * img_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-run from this point down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "simple_NN = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                    no_of_out_nodes = 10, \n",
    "                    no_of_hidden_nodes = 100,\n",
    "                    learning_rate = 0.15,\n",
    "                    activation_function = 'sigmoid', \n",
    "                    bias=None)\n",
    "    \n",
    "simple_NN.train(train_imgs, \n",
    "                train_labels_one_hot, \n",
    "                active_input_percentage=0.7, \n",
    "                active_hidden_percentage=0.7, \n",
    "                no_of_dropout_tests = 100, \n",
    "                use_dropout = False,\n",
    "                epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train:  0.71625\n",
      "accuracy test:  0.7096\n",
      "\n",
      "Training confusion matrix: \n",
      " [[4226    4   24   41    4    5  940    0    8    4]\n",
      " [  36 5666   19  124  143    0   28    0    5    0]\n",
      " [ 465   76 5123  232 2260    3 3935    0  238    2]\n",
      " [1177  204  109 5534  395    3  630    0   31    1]\n",
      " [  26   46  699   47 3180    2  342    0   21    1]\n",
      " [   2    0    1    0    0 5523    1 2551   83  770]\n",
      " [  20    0    3    3    2    0   43    0    0    0]\n",
      " [   0    0    0    0    0   72    0 3007    7   97]\n",
      " [  39    4   21   18   16  103   73   30 5593   45]\n",
      " [   9    0    1    1    0  289    8  412   14 5080]]\n",
      "\n",
      "Testing confusion matrix: \n",
      " [[687   1   5  14   0   1 147   0   1   1]\n",
      " [  7 947   2  28  22   0   6   0   1   0]\n",
      " [ 84  21 855  35 352   1 644   0  46   0]\n",
      " [202  25  18 915  60   1 131   0   4   0]\n",
      " [  4   6 114   7 561   0  50   0   2   0]\n",
      " [  0   0   0   0   0 895   0 436  21 149]\n",
      " [  4   0   0   0   0   0   7   0   0   0]\n",
      " [  0   0   0   0   0  19   0 482   1  18]\n",
      " [  8   0   6   1   4  20  15   8 922   7]\n",
      " [  4   0   0   0   1  63   0  74   2 825]]\n",
      "\n",
      "Training Label Accuracy\n",
      "digit:  0 precision:  0.7043333333333334 recall:  0.8040334855403348\n",
      "digit:  1 precision:  0.9443333333333334 recall:  0.9410396944029231\n",
      "digit:  2 precision:  0.8538333333333333 recall:  0.4153559267066645\n",
      "digit:  3 precision:  0.9223333333333333 recall:  0.6845620979713013\n",
      "digit:  4 precision:  0.53 recall:  0.7286892758936755\n",
      "digit:  5 precision:  0.9205 recall:  0.6184077930802822\n",
      "digit:  6 precision:  0.007166666666666667 recall:  0.6056338028169014\n",
      "digit:  7 precision:  0.5011666666666666 recall:  0.9447062519635564\n",
      "digit:  8 precision:  0.9321666666666667 recall:  0.941265567149108\n",
      "digit:  9 precision:  0.8466666666666667 recall:  0.8737530099759202\n",
      "\n",
      "Testing Label Accuracy\n",
      "digit:  0 precision:  0.687 recall:  0.8016336056009334\n",
      "digit:  1 precision:  0.947 recall:  0.9348469891411648\n",
      "digit:  2 precision:  0.855 recall:  0.41952894995093226\n",
      "digit:  3 precision:  0.915 recall:  0.6747787610619469\n",
      "digit:  4 precision:  0.561 recall:  0.7540322580645161\n",
      "digit:  5 precision:  0.895 recall:  0.5962691538974018\n",
      "digit:  6 precision:  0.007 recall:  0.6363636363636364\n",
      "digit:  7 precision:  0.482 recall:  0.926923076923077\n",
      "digit:  8 precision:  0.922 recall:  0.9303733602421796\n",
      "digit:  9 precision:  0.825 recall:  0.8513931888544891\n",
      "\n",
      "Training mean square error:  [3.03708333]\n",
      "Testing mean square error:  [3.1325]\n"
     ]
    }
   ],
   "source": [
    "corrects, wrongs = simple_NN.evaluate(train_imgs, train_labels)\n",
    "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = simple_NN.evaluate(test_imgs, test_labels)\n",
    "print(\"accuracy test: \", corrects / ( corrects + wrongs))\n",
    "print()\n",
    "\n",
    "train_cm = simple_NN.confusion_matrix(train_imgs, train_labels)\n",
    "print(\"Training confusion matrix: \\n\", train_cm)\n",
    "print()\n",
    "\n",
    "test_cm = simple_NN.confusion_matrix(test_imgs, test_labels)\n",
    "print(\"Testing confusion matrix: \\n\", test_cm)\n",
    "print()\n",
    "\n",
    "print(\"Training Label Accuracy\")\n",
    "for i in range(10):\n",
    "    print(\"digit: \", i, \"precision: \", simple_NN.precision(i, train_cm), \"recall: \", simple_NN.recall(i, train_cm))\n",
    "\n",
    "print()\n",
    "print(\"Testing Label Accuracy\")\n",
    "for i in range(10):\n",
    "    print(\"digit: \", i, \"precision: \", simple_NN.precision(i, test_cm), \"recall: \", simple_NN.recall(i, test_cm))\n",
    "\n",
    "print()\n",
    "train_mse = simple_NN.mean_squared_error(train_imgs, train_labels)\n",
    "print(\"Training mean square error: \", train_mse)\n",
    "\n",
    "test_mse = simple_NN.mean_squared_error(test_imgs, test_labels)\n",
    "print(\"Testing mean square error: \", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
