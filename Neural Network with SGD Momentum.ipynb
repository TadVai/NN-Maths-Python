{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Run the program from top to bottom. Download the pkl_quickdraw.pkl from the Google Drive or run Repackaging Quickdraw MNIST.ipynb with the .npy files from numpy_quickdraw_indv.zip on the Google Drive.\n",
    "\n",
    "#### Google Drive\n",
    "https://drive.google.com/drive/folders/1s2Ni2n0RjN2gfEagrqMHpTigm511qxLO?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import zipfile as zp\n",
    "import os\n",
    "from scipy.stats import truncnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Momentum\n",
    "\n",
    "SGD Momentum is performed after each call of the train_single function where the momentum value is applied to the weights.\n",
    "\n",
    "With external help from:\n",
    "https://github.com/ngailapdi/MNIST_numpy/blob/master/MNIST_SGD_Numpy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "# Activation Functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def dsigmoid(x):\n",
    "    return x * (1 - x)\n",
    "            \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def dsoftmax(x, y):\n",
    "    si_sj = -x * x.reshape(y, 1)\n",
    "    s_der = np.diag(x) + si_sj\n",
    "    return s_der\n",
    "    \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def drelu(x):\n",
    "    return np.where(x <= 0, 0, 1)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return np.where(x >= 0, x, x * 0.01)\n",
    "\n",
    "def dleaky_relu(x):\n",
    "    out = np.ones_like(x)\n",
    "    out[x < 0] *= 0.01\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_batch(imgs, labels, batch_size):\n",
    "    img_batches = []\n",
    "    lbl_batches = []\n",
    "    for counter in range(0,len(imgs),batch_size):\n",
    "        img_batches.append(imgs[counter:counter+batch_size])\n",
    "        lbl_batches.append(labels[counter:counter+batch_size])\n",
    "    return np.array(img_batches), np.array(lbl_batches)\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd, \n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 activation_function,\n",
    "                 learning_rate,\n",
    "                 bias=None,\n",
    "                ):  \n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes       \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes          \n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        \n",
    "        if activation_function == 'sigmoid':\n",
    "            self.activation_ih = sigmoid\n",
    "            self.activation_ho = sigmoid\n",
    "            self.dactivation_oh = dsigmoid\n",
    "            self.dactivation_hi = dsigmoid\n",
    "            \n",
    "        if activation_function == 'softmax':\n",
    "            self.activation_ih = sigmoid\n",
    "            self.activation_ho = softmax\n",
    "            self.dactivation_oh = dsoftmax\n",
    "            self.dactivation_hi = dsigmoid\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation_ih = relu\n",
    "            self.activation_ho = relu\n",
    "            self.dactivation_oh = drelu\n",
    "            self.dactivation_hi = drelu\n",
    "            \n",
    "        if activation_function == 'leakyrelu':\n",
    "            self.activation_ih = leaky_relu\n",
    "            self.activation_ho = leaky_relu\n",
    "            self.dactivation_oh = dleaky_relu\n",
    "            self.dactivation_hi = dleaky_relu\n",
    "        \n",
    "        self.use_dropout = False\n",
    "        self.active_input_percentage = 1.0\n",
    "        self.active_hidden_percentage = 1.0\n",
    "            \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "        self.v0 = np.zeros(self.wih.shape)\n",
    "        self.v1 = np.zeros(self.who.shape)\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "\n",
    "        n = (self.no_of_in_nodes + bias_node) * self.no_of_hidden_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.wih = X.rvs(n).reshape((self.no_of_hidden_nodes, \n",
    "                                                   self.no_of_in_nodes + bias_node))\n",
    "\n",
    "        n = (self.no_of_hidden_nodes + bias_node) * self.no_of_out_nodes\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        self.who = X.rvs(n).reshape((self.no_of_out_nodes, \n",
    "                                                    (self.no_of_hidden_nodes + bias_node)))\n",
    "\n",
    "    def dropout_weight_matrices(self,\n",
    "                                active_input_percentage=1.0,\n",
    "                                active_hidden_percentage=1.0):\n",
    "        # restore wih array, if it had been used for dropout\n",
    "        self.wih_orig = self.wih.copy()\n",
    "        self.no_of_in_nodes_orig = self.no_of_in_nodes\n",
    "        self.no_of_hidden_nodes_orig = self.no_of_hidden_nodes\n",
    "        self.who_orig = self.who.copy()\n",
    "        \n",
    "\n",
    "        active_input_nodes = int(self.no_of_in_nodes * active_input_percentage)\n",
    "        active_input_indices = sorted(random.sample(range(0, self.no_of_in_nodes), \n",
    "                                      active_input_nodes))\n",
    "        active_hidden_nodes = int(self.no_of_hidden_nodes * active_hidden_percentage)\n",
    "        active_hidden_indices = sorted(random.sample(range(0, self.no_of_hidden_nodes), \n",
    "                                       active_hidden_nodes))\n",
    "        \n",
    "        self.wih = self.wih[:, active_input_indices][active_hidden_indices]       \n",
    "        self.who = self.who[:, active_hidden_indices]\n",
    "        \n",
    "        self.no_of_hidden_nodes = active_hidden_nodes\n",
    "        self.no_of_in_nodes = active_input_nodes\n",
    "        return active_input_indices, active_hidden_indices\n",
    "    \n",
    "    def weight_matrices_reset(self, \n",
    "                              active_input_indices, \n",
    "                              active_hidden_indices):\n",
    "        \n",
    "        # resets weight matrices back to original\n",
    "        # self.wih and self.who variables are updated from the active nodes\n",
    " \n",
    "        temp = self.wih_orig.copy()[:,active_input_indices]\n",
    "        temp[active_hidden_indices] = self.wih\n",
    "        self.wih_orig[:, active_input_indices] = temp\n",
    "        self.wih = self.wih_orig.copy()\n",
    "\n",
    "        self.who_orig[:, active_hidden_indices] = self.who\n",
    "        self.who = self.who_orig.copy()\n",
    "        self.no_of_in_nodes = self.no_of_in_nodes_orig\n",
    "        self.no_of_hidden_nodes = self.no_of_hidden_nodes_orig\n",
    "        \n",
    "    \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        # Forward Propagation\n",
    "        # Input Layer to Hidden Layer\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "\n",
    "        output_vector1 = np.dot(self.wih, input_vector) # linear\n",
    "        output_vector_hidden = self.activation_ih(output_vector1) # non-linear\n",
    "        \n",
    "        # Hidden Layer to Output Layer\n",
    "        if self.bias:\n",
    "            output_vector_hidden = np.concatenate( (output_vector_hidden, [[self.bias]]) )\n",
    "        \n",
    "        output_vector2 = np.dot(self.who, output_vector_hidden) # linear\n",
    "        output_vector_network = self.activation_ho(output_vector2) # non-linear\n",
    "        \n",
    "        # Backward Propagation\n",
    "        # Output Layer to Hidden Layer\n",
    "        output_errors = target_vector - output_vector_network\n",
    "        \n",
    "        # if using softmax\n",
    "        if self.dactivation_oh == dsoftmax:\n",
    "            ovn = output_vector_network.reshape(output_vector_network.size,)\n",
    "            dsoftmax_output = dsoftmax(ovn, self.no_of_out_nodes)\n",
    "            tmp = np.dot(dsoftmax_output, output_errors)\n",
    "            \n",
    "        else:\n",
    "            tmp = output_errors * self.dactivation_oh(output_vector_network) # derivative\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            tmp = tmp / self.active_hidden_percentage # Inverted Dropout\n",
    "            \n",
    "        delta_w1 = np.dot(tmp, output_vector_hidden.T)\n",
    "\n",
    "        # Hidden Layer to Input Layer\n",
    "        # calculate hidden errors:\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        # update the weights:\n",
    "        tmp = hidden_errors * self.dactivation_hi(output_vector_hidden) # derivative\n",
    "        \n",
    "        if self.use_dropout:\n",
    "            tmp = tmp / self.active_input_percentage # Inverted Dropout\n",
    "            \n",
    "        if self.bias:\n",
    "            delta_w0 = np.dot(tmp, input_vector.T)[:-1,:] \n",
    "        else:\n",
    "            delta_w0 = np.dot(tmp, input_vector.T)\n",
    "        \n",
    "        # Momentum\n",
    "        if not self.use_momentum:\n",
    "            self.wih += self.learning_rate * delta_w0\n",
    "            self.who += self.learning_rate * delta_w1\n",
    "        else:\n",
    "            self.v0 = self.momentum * self.v0 + (1 - self.momentum) * delta_w0\n",
    "            self.v1 = self.momentum * self.v1 + (1 - self.momentum) * delta_w1\n",
    "            self.wih += self.learning_rate * self.v0\n",
    "            self.who += self.learning_rate * self.v1\n",
    "        \n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              momentum=1.0,\n",
    "              epochs=1,\n",
    "              active_input_percentage=1.0,\n",
    "              active_hidden_percentage=1.0,\n",
    "              no_of_dropout_tests = 1, \n",
    "              use_dropout = False, \n",
    "              use_momentum = False):\n",
    "        \n",
    "        self.momentum = momentum\n",
    "        self.use_momentum = use_momentum\n",
    "        \n",
    "        self.use_dropout = use_dropout\n",
    "        self.active_input_percentage = active_input_percentage\n",
    "        self.active_hidden_percentage = active_hidden_percentage\n",
    "\n",
    "        partition_length = int(len(data_array) / no_of_dropout_tests)\n",
    "        \n",
    "        # if using dropout\n",
    "        if self.use_dropout:\n",
    "            for epoch in range(epochs):\n",
    "                print(\"epoch: \", epoch)\n",
    "                for start in range(0, len(data_array), partition_length):\n",
    "                    active_in_indices, active_hidden_indices = \\\n",
    "                               self.dropout_weight_matrices(active_input_percentage,\n",
    "                                                            active_hidden_percentage)\n",
    "                    for i in range(start, start + partition_length):\n",
    "                        self.train_single(data_array[i][active_in_indices], \n",
    "                                         labels_one_hot_array[i]) \n",
    "\n",
    "                    self.weight_matrices_reset(active_in_indices, active_hidden_indices)\n",
    "        else:\n",
    "            for epoch in range(epochs):\n",
    "                print(\"epoch: \", epoch)\n",
    "                \n",
    "                for i in range(len(data_array)):\n",
    "                        self.train_single(data_array[i], labels_one_hot_array[i])\n",
    "                        \n",
    "                \n",
    "                    \n",
    "        \n",
    "    def confusion_matrix(self, data_array, labels):\n",
    "        cm = np.zeros((10, 10), int)\n",
    "        for i in range(len(data_array)):\n",
    "            res = self.run(data_array[i])\n",
    "            res_max = res.argmax()\n",
    "            target = labels[i][0]\n",
    "            cm[res_max, int(target)] += 1\n",
    "        return cm\n",
    "    \n",
    "    def precision(self, label, confusion_matrix):\n",
    "        col = confusion_matrix[:, label]\n",
    "        return confusion_matrix[label, label] / col.sum()\n",
    "    \n",
    "    def recall(self, label, confusion_matrix):\n",
    "        row = confusion_matrix[label, :]\n",
    "        return confusion_matrix[label, label] / row.sum()\n",
    "        \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        \"\"\" input_vector can be tuple, list or ndarray \"\"\"\n",
    "        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        output_vector = np.dot(self.wih, \n",
    "                               input_vector)\n",
    "        output_vector = self.activation_ih(output_vector)\n",
    "        \n",
    "        output_vector = np.dot(self.who, \n",
    "                               output_vector)\n",
    "        output_vector = self.activation_ho(output_vector)\n",
    "    \n",
    "        return output_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs\n",
    "    \n",
    "    def mean_squared_error(self, data, labels):\n",
    "        actual = labels\n",
    "        predicted = [None] * len(actual)\n",
    "        \n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            predicted[i] = res_max\n",
    "        \n",
    "        sum_square_error = 0.0\n",
    "        for i in range(len(actual)):\n",
    "            sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "        mean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "        return mean_square_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Pickle file\n",
    "with open(os.path.join(\".\",\"pkl_quickdraw.pkl\"), \"br\") as fh:\n",
    "    data = pickle.load(fh)\n",
    "train_imgs = data[0]\n",
    "test_imgs = data[1]\n",
    "train_labels = data[2]\n",
    "test_labels = data[3]\n",
    "train_labels_one_hot = data[4]\n",
    "test_labels_one_hot = data[5]\n",
    "\n",
    "img_size = 28 # dimensions\n",
    "num_of_labels = 10 # 0, 1, 2, ... 9\n",
    "image_pixels = img_size * img_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n"
     ]
    }
   ],
   "source": [
    "# Run 100 times\n",
    "epochs = 10\n",
    "\n",
    "NN = NeuralNetwork(no_of_in_nodes = image_pixels, \n",
    "                    no_of_out_nodes = 10, \n",
    "                    no_of_hidden_nodes = 100,\n",
    "                    learning_rate = 0.1,\n",
    "                    activation_function = 'softmax', \n",
    "                    bias=None)\n",
    "\n",
    "NN.train(train_imgs,\n",
    "         train_labels_one_hot,\n",
    "         epochs = epochs,  \n",
    "         momentum = 0.1, \n",
    "         active_input_percentage = 1.0, \n",
    "         active_hidden_percentage = 1.0, \n",
    "         no_of_dropout_tests = 100,\n",
    "         use_dropout = False, \n",
    "         use_momentum = True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train:  0.7083166666666667\n",
      "accuracy test:  0.7014\n",
      "\n",
      "Training confusion matrix: \n",
      " [[5234   37  153  588   25    5 1730    0   25   11]\n",
      " [  40 5737   21  185   42    0   23    0    6    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0]\n",
      " [ 346  127   58 4794  265    3  237    0   28    3]\n",
      " [  24    7  835  288 3562    0  899    0   15    0]\n",
      " [   6    0    2    0    0 5222    0  136    8   67]\n",
      " [ 313   90 4906  133 2092    0 3048    0  247    0]\n",
      " [   3    0    1    1    0  355    0 5535   22  238]\n",
      " [  34    2   23   11   14  176   62  267 5648 1962]\n",
      " [   0    0    1    0    0  239    1   62    1 3719]]\n",
      "\n",
      "Testing confusion matrix: \n",
      " [[864  11  30  81   1   2 309   0   5   2]\n",
      " [ 11 960   4  43   8   0   4   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 58  11   9 805  43   0  40   0   2   0]\n",
      " [  3   1 140  41 602   0 152   0   4   0]\n",
      " [  0   0   0   0   0 844   0  26   3  11]\n",
      " [ 53  17 811  27 341   0 481   0  46   0]\n",
      " [  0   0   0   1   0  68   0 905   3  42]\n",
      " [ 10   0   6   2   5  33  14  58 937 329]\n",
      " [  1   0   0   0   0  53   0  11   0 616]]\n",
      "\n",
      "Training Label Accuracy\n",
      "digit:  0 precision:  0.8723333333333333 recall:  0.6703381147540983\n",
      "digit:  1 precision:  0.9561666666666667 recall:  0.9476379253386191\n",
      "digit:  2 precision:  0.0 recall:  nan\n",
      "digit:  3 precision:  0.799 recall:  0.8179491554342262\n",
      "digit:  4 precision:  0.5936666666666667 recall:  0.6326820603907638\n",
      "digit:  5 precision:  0.8703333333333333 recall:  0.9597500459474362\n",
      "digit:  6 precision:  0.508 recall:  0.28146643272693694\n",
      "digit:  7 precision:  0.9225 recall:  0.8992688870836718\n",
      "digit:  8 precision:  0.9413333333333334 recall:  0.6888644956702037\n",
      "digit:  9 precision:  0.6198333333333333 recall:  0.9244345016157096\n",
      "\n",
      "Testing Label Accuracy\n",
      "digit:  0 precision:  0.864 recall:  0.6620689655172414\n",
      "digit:  1 precision:  0.96 recall:  0.9320388349514563\n",
      "digit:  2 precision:  0.0 recall:  nan\n",
      "digit:  3 precision:  0.805 recall:  0.8316115702479339\n",
      "digit:  4 precision:  0.602 recall:  0.6383881230116649\n",
      "digit:  5 precision:  0.844 recall:  0.9547511312217195\n",
      "digit:  6 precision:  0.481 recall:  0.2708333333333333\n",
      "digit:  7 precision:  0.905 recall:  0.888125613346418\n",
      "digit:  8 precision:  0.937 recall:  0.6721664275466284\n",
      "digit:  9 precision:  0.616 recall:  0.9045521292217328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jly09\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:272: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean square error:  [3.4482]\n",
      "Testing mean square error:  [3.5814]\n"
     ]
    }
   ],
   "source": [
    "corrects, wrongs = NN.evaluate(train_imgs, train_labels)\n",
    "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = NN.evaluate(test_imgs, test_labels)\n",
    "print(\"accuracy test: \", corrects / ( corrects + wrongs))\n",
    "print()\n",
    "\n",
    "train_cm = NN.confusion_matrix(train_imgs, train_labels)\n",
    "print(\"Training confusion matrix: \\n\", train_cm)\n",
    "print()\n",
    "\n",
    "test_cm = NN.confusion_matrix(test_imgs, test_labels)\n",
    "print(\"Testing confusion matrix: \\n\", test_cm)\n",
    "print()\n",
    "\n",
    "print(\"Training Label Accuracy\")\n",
    "for i in range(10):\n",
    "    print(\"digit: \", i, \"precision: \", NN.precision(i, train_cm), \"recall: \", NN.recall(i, train_cm))\n",
    "\n",
    "print()\n",
    "print(\"Testing Label Accuracy\")\n",
    "for i in range(10):\n",
    "    print(\"digit: \", i, \"precision: \", NN.precision(i, test_cm), \"recall: \", NN.recall(i, test_cm))\n",
    "\n",
    "print()\n",
    "train_mse = NN.mean_squared_error(train_imgs, train_labels)\n",
    "print(\"Training mean square error: \", train_mse)\n",
    "\n",
    "test_mse = NN.mean_squared_error(test_imgs, test_labels)\n",
    "print(\"Testing mean square error: \", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
